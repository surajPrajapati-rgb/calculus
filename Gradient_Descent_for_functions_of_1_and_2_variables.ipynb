{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoPsDi7cpIJC3LBmmAGwEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suraj-07-coder/calculus/blob/main/Gradient_Descent_for_functions_of_1_and_2_variables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "g4JyuSIoqvwi"
      },
      "outputs": [],
      "source": [
        "import autograd.numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from autograd import grad\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_function(x):\n",
        "  return np.sin(x)\n",
        "gradient_func = grad(my_function)\n",
        "gradient = gradient_func(2.0)\n",
        "print(gradient)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjzUpkF9regZ",
        "outputId": "91707837-7782-42bd-afa9-8176a5bc4766"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.4161468365471424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def myfunc(X):\n",
        "  x,y = X\n",
        "  return x**2 + y**2\n",
        "gradient_func = grad(myfunc)\n",
        "gradient = gradient_func([3.0,2.0])\n",
        "print(gradient)\n",
        "print(type(gradient))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4PAYbZIrlBj",
        "outputId": "617276d1-7ce4-4bb6-ad7e-cf4bf6ebca36"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array(6.), array(4.)]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SYMBOLIC DERIVATIVE, AUTOMATIC DERIVATIVE, NUMERICAL DERIVATIVE"
      ],
      "metadata": {
        "id": "eQKploj1wPTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# creating a x list from -10 to 10\n",
        "xlist = [i/10 for i in range(-100,100)]\n",
        "\n",
        "# define a function that returns the sine of an input value\n",
        "def my_function(x):\n",
        "  return np.sin(x)\n",
        "\n",
        "# create an empty list to store calculated values\n",
        "\n",
        "# create empty lists to store calculated values\n",
        "y_num = []\n",
        "y_sym = []\n",
        "y_auto = []\n",
        "\n",
        "# set the time step\n",
        "dt = 0.09\n",
        "\n",
        "# loop through each element in the xlist\n",
        "for i in range(len(xlist)):\n",
        "\n",
        "  # calculate the symbolic derivative of cos(x)\n",
        "  y_sym.append(math.cos(xlist[i]))\n",
        "\n",
        "  # calculate the numerical derivative of sin(x)\n",
        "  yn = (math.sin(xlist[i] + dt) - math.sin(xlist[i])) / dt\n",
        "\n",
        "  # calculate the gradient of my_function using autograd's grad function\n",
        "  gradient_func = grad(my_function)\n",
        "  gradient = gradient_func(xlist[i]) \n",
        "\n",
        "  # add the calculated numerical derivative to y_num list\n",
        "  y_num.append(yn)\n",
        "\n",
        "  # add the calculated gradient to y_auto list\n",
        "  y_auto.append(gradient)\n"
      ],
      "metadata": {
        "id": "SaVZTyKAwY0u"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(name = 'Numerical', x = xlist, y = y_num))\n",
        "fig.add_trace(go.Scatter(name = 'Symbolic', x = xlist, y = y_sym))\n",
        "fig.add_trace(go.Scatter(name = 'Automatic', x = xlist, y = y_auto))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "FpPIjTCrvpmN",
        "outputId": "d7ea3762-5207-4417-e09d-5d87a51171e3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"ee6d15d9-50b2-466b-9d40-f555b0a5513a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ee6d15d9-50b2-466b-9d40-f555b0a5513a\")) {                    Plotly.newPlot(                        \"ee6d15d9-50b2-466b-9d40-f555b0a5513a\",                        [{\"name\":\"Numerical\",\"x\":[-10.0,-9.9,-9.8,-9.7,-9.6,-9.5,-9.4,-9.3,-9.2,-9.1,-9.0,-8.9,-8.8,-8.7,-8.6,-8.5,-8.4,-8.3,-8.2,-8.1,-8.0,-7.9,-7.8,-7.7,-7.6,-7.5,-7.4,-7.3,-7.2,-7.1,-7.0,-6.9,-6.8,-6.7,-6.6,-6.5,-6.4,-6.3,-6.2,-6.1,-6.0,-5.9,-5.8,-5.7,-5.6,-5.5,-5.4,-5.3,-5.2,-5.1,-5.0,-4.9,-4.8,-4.7,-4.6,-4.5,-4.4,-4.3,-4.2,-4.1,-4.0,-3.9,-3.8,-3.7,-3.6,-3.5,-3.4,-3.3,-3.2,-3.1,-3.0,-2.9,-2.8,-2.7,-2.6,-2.5,-2.4,-2.3,-2.2,-2.1,-2.0,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1.0,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,3.0,3.1,3.2,3.3,3.4,3.5,3.6,3.7,3.8,3.9,4.0,4.1,4.2,4.3,4.4,4.5,4.6,4.7,4.8,4.9,5.0,5.1,5.2,5.3,5.4,5.5,5.6,5.7,5.8,5.9,6.0,6.1,6.2,6.3,6.4,6.5,6.6,6.7,6.8,6.9,7.0,7.1,7.2,7.3,7.4,7.5,7.6,7.7,7.8,7.9,8.0,8.1,8.2,8.3,8.4,8.5,8.6,8.7,8.8,8.9,9.0,9.1,9.2,9.3,9.4,9.5,9.6,9.7,9.8,9.9],\"y\":[-0.8624036709958767,-0.9085664519597674,-0.9456511372678149,-0.9732871890029894,-0.9911984768716154,-0.9992060372059948,-0.9972298611102147,-0.9852896938825866,-0.9635048377271542,-0.9320929597255069,-0.8913679169792509,-0.841736620653598,-0.783694970255507,-0.7178228987697377,-0.6447785781601668,-0.565291843133078,-0.4801568988700446,-0.39022438559226996,-0.2963928792446866,-0.19959991322215315,-0.10081261084566245,-0.0010180221858218591,0.09878673821518907,0.19760445418250455,0.29444777176297837,0.38834906453949,0.47837010183425543,0.5636114231996244,0.6432213255295502,0.7164043729954878,0.7824293447782522,0.8406365411847484,0.8904443751492241,0.9313551832589837,0.9629601982427127,0.9849436332379203,0.9970858370288933,0.9992654887290312,0.9914608099790428,0.9737497825491144,0.9463093691708385,0.9094137453840967,0.8634315600656935,0.8088222520116416,0.7461314593765768,0.6759855678376941,0.5990854519560952,0.5161994722698737,0.4281557980895899,0.3358341327042849,0.24015692367700528,0.14208014605366792,0.04258375057641434,-0.057338127660280486,-0.15668710227886173,-0.25447051116534014,-0.34971133482102223,-0.4414579584183725,-0.5287936800218046,-0.6108458699704099,-0.6867946899050693,-0.7558812843223388,-0.8174153628077916,-0.8707820971896651,-0.9154482646987101,-0.9509675757538497,-0.9769851331401433,-0.9932409780244493,-0.9995726873781505,-0.99591699685437,-0.9823104329044109,-0.9588889478175289,-0.9258865613305985,-0.8836330223802591,-0.8325505143605392,-0.7731494368059385,-0.7060233056479983,-0.6318428230002998,-0.5513491757246507,-0.46534662973696855,-0.37469449404809885,-0.28029853483223266,-0.18310192531069877,-0.08407582187691048,0.015790339377299813,0.11549872878004526,0.21405309306362394,0.31046870959785655,0.4037822254130963,0.49306128270466915,0.5774138346438449,0.6559970584148984,0.7280257764220674,0.7927803015245526,0.8496136279125321,0.897957895775334,0.9373300651689663,0.9673367423915555,0.9876781106431821,0.9981509256962386,0.9986505466445671,0.9891719814408046,0.9698099367752708,0.940757871798021,0.9023060651389723,0.8548387145397863,0.7988300980770278,0.7348398353324077,0.6635072958589006,0.5855452108115234,0.5017325515732307,0.412906746530348,0.3199553137649599,0.22380699326759582,0.12542246727425851,0.02578476144707113,-0.07411057719318467,-0.17326542744382625,-0.27068906681738547,-0.3654080705132142,-0.456476037556323,-0.5429830469230857,-0.6240647491713269,-0.6989110027342248,-0.7667739685870657,-0.8269755824075606,-0.8789143295704208,-0.9220712552826644,-0.9560151498083567,-0.9804068569737572,-0.9950026629036953,-0.9996567311300516,-0.9943225597415382,-0.9790534460154265,-0.9540019538887705,-0.9194183895899766,-0.8756483006617115,-0.82312902336408,-0.7623853129552753,-0.6940241005104992,-0.6187284286672898,-0.5372506268892617,-0.4504047944388017,-0.35905866616634163,-0.26412494239056084,-0.16655216949851281,-0.0673152623836616,0.032594236581458734,0.13217806470887908,0.2304412133059899,0.3264018694734817,0.4191012260553086,0.5076130617228374,0.5910529954722016,0.6685873230669517,0.739441347135202,0.8029071176896897,0.8583505057300279,0.905217539250067,0.9430399383430543,0.9714397940996761,0.9901333445489565,0.998933809914073,0.997753258854144,0.9866034870451219,0.9655958993212983,0.9349403965550236,0.8949432783965773,0.8460041828293093,0.7886120931190121,0.7233404520547709,0.650841432298161,0.5718394200896025,0.4871237774204889,0.3975409549891013,0.3039860347450321,0.20739378652621593,0.10872932814769791,0.008978482263466425,-0.09086207364764991,-0.18979476575388615,-0.28683109129851786,-0.3810014953926485,-0.4713650584871671,-0.5570188977298527,-0.63710718827239,-0.7108297143893457,-0.7774498649691869,-0.8363019934890144,-0.8867980689345831,-0.9284335512118269,-0.9607924323446901,-0.9835513930893177,-0.9964830334330625,-0.9994581447002407,-0.9924470005625124,-0.9755196540545251,-0.9488452376271501,-0.9126902732319395,-0.8674160093218889],\"type\":\"scatter\"},{\"name\":\"Symbolic\",\"x\":[-10.0,-9.9,-9.8,-9.7,-9.6,-9.5,-9.4,-9.3,-9.2,-9.1,-9.0,-8.9,-8.8,-8.7,-8.6,-8.5,-8.4,-8.3,-8.2,-8.1,-8.0,-7.9,-7.8,-7.7,-7.6,-7.5,-7.4,-7.3,-7.2,-7.1,-7.0,-6.9,-6.8,-6.7,-6.6,-6.5,-6.4,-6.3,-6.2,-6.1,-6.0,-5.9,-5.8,-5.7,-5.6,-5.5,-5.4,-5.3,-5.2,-5.1,-5.0,-4.9,-4.8,-4.7,-4.6,-4.5,-4.4,-4.3,-4.2,-4.1,-4.0,-3.9,-3.8,-3.7,-3.6,-3.5,-3.4,-3.3,-3.2,-3.1,-3.0,-2.9,-2.8,-2.7,-2.6,-2.5,-2.4,-2.3,-2.2,-2.1,-2.0,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1.0,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,3.0,3.1,3.2,3.3,3.4,3.5,3.6,3.7,3.8,3.9,4.0,4.1,4.2,4.3,4.4,4.5,4.6,4.7,4.8,4.9,5.0,5.1,5.2,5.3,5.4,5.5,5.6,5.7,5.8,5.9,6.0,6.1,6.2,6.3,6.4,6.5,6.6,6.7,6.8,6.9,7.0,7.1,7.2,7.3,7.4,7.5,7.6,7.7,7.8,7.9,8.0,8.1,8.2,8.3,8.4,8.5,8.6,8.7,8.8,8.9,9.0,9.1,9.2,9.3,9.4,9.5,9.6,9.7,9.8,9.9],\"y\":[-0.8390715290764524,-0.8891911526253609,-0.9304262721047533,-0.9623648798313102,-0.984687855794127,-0.9971721561963784,-0.9996930420352065,-0.9922253254526034,-0.9748436214041636,-0.9477216021311119,-0.9111302618846769,-0.8654352092411123,-0.811093014061656,-0.7486466455973987,-0.6787200473200125,-0.6020119026848236,-0.5192886541166856,-0.4313768449706208,-0.3391548609838345,-0.2435441537357911,-0.14550003380861354,-0.04600212563953695,0.05395542056264975,0.15337386203786435,0.2512598425822557,0.3466353178350258,0.43854732757439036,0.5260775173811053,0.6083513145322546,0.6845466664428066,0.7539022543433046,0.8157251001253568,0.8693974903498253,0.9143831482353194,0.9502325919585296,0.9765876257280235,0.9931849187581926,0.9998586363834151,0.9965420970232175,0.9832684384425845,0.960170286650366,0.9274784307440359,0.8855195169413189,0.8347127848391598,0.7755658785102496,0.70866977429126,0.6346928759426347,0.5543743361791608,0.4685166713003771,0.37797774271298024,0.28366218546322625,0.18651236942257576,0.0874989834394464,-0.01238866346289056,-0.11215252693505487,-0.2107957994307797,-0.30733286997841935,-0.40079917207997545,-0.4902608213406994,-0.5748239465332692,-0.6536436208636119,-0.7259323042001402,-0.7909677119144168,-0.848100031710408,-0.896758416334147,-0.9364566872907963,-0.9667981925794611,-0.9874797699088649,-0.9982947757947531,-0.9991351502732795,-0.9899924966004454,-0.9709581651495905,-0.9422223406686581,-0.9040721420170612,-0.8568887533689473,-0.8011436155469337,-0.7373937155412454,-0.6662760212798241,-0.5885011172553458,-0.5048461045998576,-0.4161468365471424,-0.32328956686350335,-0.2272020946930871,-0.12884449429552464,-0.029199522301288815,0.0707372016677029,0.16996714290024104,0.26749882862458735,0.3623577544766736,0.4535961214255773,0.5403023058681398,0.6216099682706644,0.6967067093471654,0.7648421872844885,0.8253356149096783,0.8775825618903728,0.9210609940028851,0.955336489125606,0.9800665778412416,0.9950041652780258,1.0,0.9950041652780258,0.9800665778412416,0.955336489125606,0.9210609940028851,0.8775825618903728,0.8253356149096783,0.7648421872844885,0.6967067093471654,0.6216099682706644,0.5403023058681398,0.4535961214255773,0.3623577544766736,0.26749882862458735,0.16996714290024104,0.0707372016677029,-0.029199522301288815,-0.12884449429552464,-0.2272020946930871,-0.32328956686350335,-0.4161468365471424,-0.5048461045998576,-0.5885011172553458,-0.6662760212798241,-0.7373937155412454,-0.8011436155469337,-0.8568887533689473,-0.9040721420170612,-0.9422223406686581,-0.9709581651495905,-0.9899924966004454,-0.9991351502732795,-0.9982947757947531,-0.9874797699088649,-0.9667981925794611,-0.9364566872907963,-0.896758416334147,-0.848100031710408,-0.7909677119144168,-0.7259323042001402,-0.6536436208636119,-0.5748239465332692,-0.4902608213406994,-0.40079917207997545,-0.30733286997841935,-0.2107957994307797,-0.11215252693505487,-0.01238866346289056,0.0874989834394464,0.18651236942257576,0.28366218546322625,0.37797774271298024,0.4685166713003771,0.5543743361791608,0.6346928759426347,0.70866977429126,0.7755658785102496,0.8347127848391598,0.8855195169413189,0.9274784307440359,0.960170286650366,0.9832684384425845,0.9965420970232175,0.9998586363834151,0.9931849187581926,0.9765876257280235,0.9502325919585296,0.9143831482353194,0.8693974903498253,0.8157251001253568,0.7539022543433046,0.6845466664428066,0.6083513145322546,0.5260775173811053,0.43854732757439036,0.3466353178350258,0.2512598425822557,0.15337386203786435,0.05395542056264975,-0.04600212563953695,-0.14550003380861354,-0.2435441537357911,-0.3391548609838345,-0.4313768449706208,-0.5192886541166856,-0.6020119026848236,-0.6787200473200125,-0.7486466455973987,-0.811093014061656,-0.8654352092411123,-0.9111302618846769,-0.9477216021311119,-0.9748436214041636,-0.9922253254526034,-0.9996930420352065,-0.9971721561963784,-0.984687855794127,-0.9623648798313102,-0.9304262721047533,-0.8891911526253609],\"type\":\"scatter\"},{\"name\":\"Automatic\",\"x\":[-10.0,-9.9,-9.8,-9.7,-9.6,-9.5,-9.4,-9.3,-9.2,-9.1,-9.0,-8.9,-8.8,-8.7,-8.6,-8.5,-8.4,-8.3,-8.2,-8.1,-8.0,-7.9,-7.8,-7.7,-7.6,-7.5,-7.4,-7.3,-7.2,-7.1,-7.0,-6.9,-6.8,-6.7,-6.6,-6.5,-6.4,-6.3,-6.2,-6.1,-6.0,-5.9,-5.8,-5.7,-5.6,-5.5,-5.4,-5.3,-5.2,-5.1,-5.0,-4.9,-4.8,-4.7,-4.6,-4.5,-4.4,-4.3,-4.2,-4.1,-4.0,-3.9,-3.8,-3.7,-3.6,-3.5,-3.4,-3.3,-3.2,-3.1,-3.0,-2.9,-2.8,-2.7,-2.6,-2.5,-2.4,-2.3,-2.2,-2.1,-2.0,-1.9,-1.8,-1.7,-1.6,-1.5,-1.4,-1.3,-1.2,-1.1,-1.0,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,3.0,3.1,3.2,3.3,3.4,3.5,3.6,3.7,3.8,3.9,4.0,4.1,4.2,4.3,4.4,4.5,4.6,4.7,4.8,4.9,5.0,5.1,5.2,5.3,5.4,5.5,5.6,5.7,5.8,5.9,6.0,6.1,6.2,6.3,6.4,6.5,6.6,6.7,6.8,6.9,7.0,7.1,7.2,7.3,7.4,7.5,7.6,7.7,7.8,7.9,8.0,8.1,8.2,8.3,8.4,8.5,8.6,8.7,8.8,8.9,9.0,9.1,9.2,9.3,9.4,9.5,9.6,9.7,9.8,9.9],\"y\":[-0.8390715290764524,-0.8891911526253609,-0.9304262721047533,-0.9623648798313102,-0.984687855794127,-0.9971721561963784,-0.9996930420352065,-0.9922253254526034,-0.9748436214041636,-0.9477216021311119,-0.9111302618846769,-0.8654352092411123,-0.811093014061656,-0.7486466455973987,-0.6787200473200125,-0.6020119026848236,-0.5192886541166856,-0.4313768449706208,-0.3391548609838345,-0.2435441537357911,-0.14550003380861354,-0.04600212563953695,0.05395542056264975,0.15337386203786435,0.2512598425822557,0.3466353178350258,0.43854732757439036,0.5260775173811053,0.6083513145322546,0.6845466664428066,0.7539022543433046,0.8157251001253568,0.8693974903498253,0.9143831482353194,0.9502325919585296,0.9765876257280235,0.9931849187581926,0.9998586363834151,0.9965420970232175,0.9832684384425845,0.960170286650366,0.9274784307440359,0.8855195169413189,0.8347127848391598,0.7755658785102496,0.70866977429126,0.6346928759426347,0.5543743361791608,0.4685166713003771,0.37797774271298024,0.28366218546322625,0.18651236942257576,0.0874989834394464,-0.01238866346289056,-0.11215252693505487,-0.2107957994307797,-0.30733286997841935,-0.40079917207997545,-0.4902608213406994,-0.5748239465332692,-0.6536436208636119,-0.7259323042001402,-0.7909677119144168,-0.848100031710408,-0.896758416334147,-0.9364566872907963,-0.9667981925794611,-0.9874797699088649,-0.9982947757947531,-0.9991351502732795,-0.9899924966004454,-0.9709581651495905,-0.9422223406686581,-0.9040721420170612,-0.8568887533689473,-0.8011436155469337,-0.7373937155412454,-0.6662760212798241,-0.5885011172553458,-0.5048461045998576,-0.4161468365471424,-0.32328956686350335,-0.2272020946930871,-0.12884449429552464,-0.029199522301288815,0.0707372016677029,0.16996714290024104,0.26749882862458735,0.3623577544766736,0.4535961214255773,0.5403023058681398,0.6216099682706644,0.6967067093471654,0.7648421872844885,0.8253356149096783,0.8775825618903728,0.9210609940028851,0.955336489125606,0.9800665778412416,0.9950041652780258,1.0,0.9950041652780258,0.9800665778412416,0.955336489125606,0.9210609940028851,0.8775825618903728,0.8253356149096783,0.7648421872844885,0.6967067093471654,0.6216099682706644,0.5403023058681398,0.4535961214255773,0.3623577544766736,0.26749882862458735,0.16996714290024104,0.0707372016677029,-0.029199522301288815,-0.12884449429552464,-0.2272020946930871,-0.32328956686350335,-0.4161468365471424,-0.5048461045998576,-0.5885011172553458,-0.6662760212798241,-0.7373937155412454,-0.8011436155469337,-0.8568887533689473,-0.9040721420170612,-0.9422223406686581,-0.9709581651495905,-0.9899924966004454,-0.9991351502732795,-0.9982947757947531,-0.9874797699088649,-0.9667981925794611,-0.9364566872907963,-0.896758416334147,-0.848100031710408,-0.7909677119144168,-0.7259323042001402,-0.6536436208636119,-0.5748239465332692,-0.4902608213406994,-0.40079917207997545,-0.30733286997841935,-0.2107957994307797,-0.11215252693505487,-0.01238866346289056,0.0874989834394464,0.18651236942257576,0.28366218546322625,0.37797774271298024,0.4685166713003771,0.5543743361791608,0.6346928759426347,0.70866977429126,0.7755658785102496,0.8347127848391598,0.8855195169413189,0.9274784307440359,0.960170286650366,0.9832684384425845,0.9965420970232175,0.9998586363834151,0.9931849187581926,0.9765876257280235,0.9502325919585296,0.9143831482353194,0.8693974903498253,0.8157251001253568,0.7539022543433046,0.6845466664428066,0.6083513145322546,0.5260775173811053,0.43854732757439036,0.3466353178350258,0.2512598425822557,0.15337386203786435,0.05395542056264975,-0.04600212563953695,-0.14550003380861354,-0.2435441537357911,-0.3391548609838345,-0.4313768449706208,-0.5192886541166856,-0.6020119026848236,-0.6787200473200125,-0.7486466455973987,-0.811093014061656,-0.8654352092411123,-0.9111302618846769,-0.9477216021311119,-0.9748436214041636,-0.9922253254526034,-0.9996930420352065,-0.9971721561963784,-0.984687855794127,-0.9623648798313102,-0.9304262721047533,-0.8891911526253609],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ee6d15d9-50b2-466b-9d40-f555b0a5513a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent for functions of single variable"
      ],
      "metadata": {
        "id": "EALmEzOZ_uOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is a popular optimization algorithm used to find the minimum of a function. It works by iteratively updating the parameters of the function based on the negative gradient of the function with respect to those parameters. The algorithm follows the direction of steepest descent to converge towards a minimum.\n",
        "\n",
        "For a function of one variable, the gradient descent algorithm can be written as:\n",
        "$$x_{n+1} = x_n - alpha * f'(x_n)\n",
        "$$"
      ],
      "metadata": {
        "id": "fZNJPr9q4Ip7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below gradient descent algorithm  is for $$f(x) = sin(x)$$"
      ],
      "metadata": {
        "id": "IgS6CXZW-OOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = float(input('enter orbitary number: '))\n",
        "def my_function(x):\n",
        "  return np.sin(x)\n",
        "gradient_func = grad(my_function)\n",
        "initial_gradient = gradient_func(x0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUe0Q0UfFMN3",
        "outputId": "b0118b51-a511-4f82-822d-5cc7b7e3562f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter orbitary number: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEWakcu0IqLs",
        "outputId": "510f88da-62ec-4384-e1db-0811a091fcd7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.14550003380861354"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_function(x):\n",
        "  return np.sin(x)\n",
        "gradient_func = grad(my_function)\n",
        "lr = 0.01\n",
        "xn = initial_gradient\n",
        "while  abs(gradient_func(xn)) > 1e-3 :\n",
        "  xn = xn - lr*gradient_func(xn)\n",
        "print(gradient_func(xn))\n",
        "print(my_function(xn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcTQmDsox87F",
        "outputId": "aceff567-b328-4c04-f9be-886a2b1ec296"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0009940552504930033\n",
            "-0.9999995059269574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also gradient descent algorithm  is for $f(x) = x^2 + 4$"
      ],
      "metadata": {
        "id": "zVFZkifo-JAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = float(input('enter orbitary number: '))\n",
        "def my_function(x):\n",
        "  return -x**2 + 4\n",
        "gradient_func = grad(my_function)\n",
        "initial_gradient = gradient_func(x0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCaqugC_-Lhk",
        "outputId": "05227d5e-9e7a-446a-b500-9a18a8899166"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter orbitary number: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_function(x):\n",
        "  return -x**2 + 4\n",
        "gradient_func = grad(my_function)\n",
        "xn = initial_gradient\n",
        "lr = 0.1\n",
        "while  abs(gradient_func(xn)) > 1e-3 :\n",
        "  xn = xn + lr*gradient_func(xn)\n",
        "print(gradient_func(xn))\n",
        "print(my_function(xn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyrY1kHa-Sgc",
        "outputId": "2cf868f6-d015-43e9-8da6-455d69e35f61"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0008920298079412251\n",
            "3.9999998010707056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent for functions of two variable function"
      ],
      "metadata": {
        "id": "pj7YuhTXFvvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent is a popular optimization algorithm used to find the minimum value of a function. It is widely used in machine learning and artificial intelligence applications. In the case of a function of two variables, the gradient descent algorithm can be described as follows:\n",
        "\n",
        "1. Choose a starting point (x,y) and a learning rate alpha.\n",
        "2.Compute the gradient of the function at the current point (x,y). The gradient is a vector that points in the direction of the steepest increase of the function at that point. The gradient of a function of two variables f(x,y) is given by:\n",
        "∇f(x,y) = [∂f/∂x, ∂f/∂y]\n",
        "3. Update the current point by subtracting the learning rate times the gradient:\n",
        "x_new = x - alpha * ∂f/∂x\n",
        "y_new = y - alpha * ∂f/∂y\n",
        "4. Repeat steps 2 and 3 until the algorithm converges to a minimum.\n",
        "The choice of the learning rate alpha is important because it determines the step size of the algorithm. If alpha is too large, the algorithm may overshoot the minimum and fail to converge. If alpha is too small, the algorithm may take too long to converge. The optimal value of alpha depends on the specific problem and may require some experimentation.\n",
        "\n",
        "Overall, gradient descent is a powerful optimization algorithm that can be used to find the minimum value of a function of two variables."
      ],
      "metadata": {
        "id": "BZ3wbwWAFGDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient algorithm for x^2 + y^2"
      ],
      "metadata": {
        "id": "YiVYb8tqZKSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to minimize\n",
        "def myfunc(X):\n",
        "    x, y = X\n",
        "    return x**2 + y**2\n",
        "\n",
        "gradient_func = grad(myfunc)\n",
        "\n",
        "X0, Y0 = 1.0, 2.0\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Define the stopping criterion\n",
        "epsilon = 1e-6\n",
        "\n",
        "# Initialize the iteration counter and the current point\n",
        "iters = 0\n",
        "X = np.array([X0, Y0])\n",
        "\n",
        "# Run the gradient descent algorithm\n",
        "while True:\n",
        "    # Compute the gradient at the current point\n",
        "    gradient = gradient_func(X)\n",
        " \n",
        "    # Update the current point\n",
        "    X_new = X - learning_rate * gradient\n",
        "\n",
        "    # Compute the difference between the current and new points\n",
        "    diff = np.linalg.norm(X_new - X)\n",
        "\n",
        "    # Check if the stopping criterion is satisfied\n",
        "    if diff < epsilon:\n",
        "        break\n",
        "\n",
        "    # Update the iteration counter and the current point\n",
        "    iters += 1\n",
        "    X = X_new\n",
        "\n",
        "\n",
        "# Print the final result\n",
        "print(f\"The minimum point is {X} and the minimum value is {myfunc(X)} after {iters} iterations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrEAlUEGCs9B",
        "outputId": "65200ccd-e79a-4c7d-9ac9-905d6b4d0c60"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum point is [1.91561943e-06 3.83123885e-06] and the minimum value is 1.834798892792059e-11 after 59 iterations.\n"
          ]
        }
      ]
    }
  ]
}